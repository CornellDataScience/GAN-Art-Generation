{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-11-30T23:42:20.354496Z","iopub.status.busy":"2021-11-30T23:42:20.354162Z","iopub.status.idle":"2021-11-30T23:42:20.358894Z","shell.execute_reply":"2021-11-30T23:42:20.358228Z","shell.execute_reply.started":"2021-11-30T23:42:20.354462Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","import os"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:42:26.428036Z","iopub.status.busy":"2021-11-30T23:42:26.427682Z","iopub.status.idle":"2021-11-30T23:42:35.405926Z","shell.execute_reply":"2021-11-30T23:42:35.404725Z","shell.execute_reply.started":"2021-11-30T23:42:26.427998Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (Dense, \n","                                     BatchNormalization, \n","                                     LeakyReLU, \n","                                     Reshape, \n","                                     Conv2DTranspose,\n","                                     Conv2D,\n","                                     Dropout,\n","                                     Flatten)\n","import matplotlib.pyplot as plt\n","from kaggle_datasets import KaggleDatasets\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    \n","print(tf.__version__)\n","GCS_PATH = KaggleDatasets().get_gcs_path()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:42:44.380638Z","iopub.status.busy":"2021-11-30T23:42:44.380328Z","iopub.status.idle":"2021-11-30T23:42:44.542257Z","shell.execute_reply":"2021-11-30T23:42:44.541250Z","shell.execute_reply.started":"2021-11-30T23:42:44.380608Z"},"trusted":true},"outputs":[],"source":["MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n","PHOTOS = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:42:53.551172Z","iopub.status.busy":"2021-11-30T23:42:53.550267Z","iopub.status.idle":"2021-11-30T23:42:53.561111Z","shell.execute_reply":"2021-11-30T23:42:53.560196Z","shell.execute_reply.started":"2021-11-30T23:42:53.551119Z"},"trusted":true},"outputs":[],"source":["IMAGE_SIZE = [256, 256]\n","#scale to [-1,1]\n","\n","def decode_image(image):\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = (tf.cast(image, tf.float32) / 127.5) - 1\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image\n","\n","def read_tfrecord(example):\n","    tfrecord_format = {\n","        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","        \"target\": tf.io.FixedLenFeature([], tf.string)\n","    }\n","    example = tf.io.parse_single_example(example, tfrecord_format)\n","    image = decode_image(example['image'])\n","    return image\n","\n","def load_dataset(filenames, labeled=True, ordered=False):\n","    dataset = tf.data.TFRecordDataset(filenames)\n","    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n","    return dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:42:56.669439Z","iopub.status.busy":"2021-11-30T23:42:56.668715Z","iopub.status.idle":"2021-11-30T23:42:56.873521Z","shell.execute_reply":"2021-11-30T23:42:56.872351Z","shell.execute_reply.started":"2021-11-30T23:42:56.669388Z"},"trusted":true},"outputs":[],"source":["monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\n","photo_ds = load_dataset(PHOTOS, labeled=True)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:42:58.333986Z","iopub.status.busy":"2021-11-30T23:42:58.333679Z","iopub.status.idle":"2021-11-30T23:42:58.926276Z","shell.execute_reply":"2021-11-30T23:42:58.925236Z","shell.execute_reply.started":"2021-11-30T23:42:58.333956Z"},"trusted":true},"outputs":[],"source":["for element in photo_ds:\n","    x=element\n","    break"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:00.372407Z","iopub.status.busy":"2021-11-30T23:43:00.372067Z","iopub.status.idle":"2021-11-30T23:43:00.382310Z","shell.execute_reply":"2021-11-30T23:43:00.381489Z","shell.execute_reply.started":"2021-11-30T23:43:00.372368Z"},"trusted":true},"outputs":[],"source":["y=tf.image.resize(x, [256, 256]) # WAS [28,28]\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:35.386232Z","iopub.status.busy":"2021-11-30T23:43:35.385892Z","iopub.status.idle":"2021-11-30T23:43:35.397000Z","shell.execute_reply":"2021-11-30T23:43:35.396237Z","shell.execute_reply.started":"2021-11-30T23:43:35.386199Z"},"trusted":true},"outputs":[],"source":["def make_generator_model():\n","    model = tf.keras.Sequential()\n","    model.add(Dense(7*7*512, use_bias=False, input_shape=(256,))) #originally 7*7*7, 256*256\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","\n","    model.add(Reshape((7,7,512)))#was 7,7,512\n","#     assert model.output_shape == (None, 7, 7, 512) # Note: None is the batch size\n","    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)) #originally 128\n","#     assert model.output_shape == (None, 7, 7, 128)\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","    \n","    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)) #originally 64\n","#     assert model.output_shape == (None, 14, 14, 64)\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))#originally 3\n","#     print(model.output_shape)\n","#     assert model.output_shape == (None, 28, 28, 3)\n","    return model\n","    #model = tf.keras.Sequential()\n","    #model.add(Dense(7*7*512, use_bias=False, input_shape=(256*256,)))\n","    #model.add(BatchNormalization())\n","    #model.add(LeakyReLU())\n","\n","    #model.add(Reshape((7,7,512)))\n","    #assert model.output_shape == (None, 7, 7, 512) # Note: None is the batch size\n","\n","    #model.add(Conv2DTranspose(32, kernel_size=(3,3), input_shape=(28,28,3)))\n","    #assert model.output_shape == (None, 7, 7, 128)\n","    #model.add(Conv2DTranspose(64, (3,3)))\n","    #model.add(MaxPooling2D(pool_size=(2,2)))\n","    #model.add(Dropout(0.25))\n","              \n","    #model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","\n","    return model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:45.983163Z","iopub.status.busy":"2021-11-30T23:43:45.982196Z","iopub.status.idle":"2021-11-30T23:43:46.148617Z","shell.execute_reply":"2021-11-30T23:43:46.147445Z","shell.execute_reply.started":"2021-11-30T23:43:45.983116Z"},"trusted":true},"outputs":[],"source":["generator = make_generator_model()\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:46.474479Z","iopub.status.busy":"2021-11-30T23:43:46.474152Z","iopub.status.idle":"2021-11-30T23:43:46.774312Z","shell.execute_reply":"2021-11-30T23:43:46.773361Z","shell.execute_reply.started":"2021-11-30T23:43:46.474444Z"},"trusted":true},"outputs":[],"source":["noise = tf.random.normal([3, 256])*127.5+100\n","starting_image = tf.reshape(x, (3, 256*256))#y[:,:,0:3]\n","generated_image = generator(noise, training=False)\n","plt.imshow(generated_image[0, :, :, 0:3]) #formerly cmap='gray'"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:47.251895Z","iopub.status.busy":"2021-11-30T23:43:47.251021Z","iopub.status.idle":"2021-11-30T23:43:47.259459Z","shell.execute_reply":"2021-11-30T23:43:47.258537Z","shell.execute_reply.started":"2021-11-30T23:43:47.251853Z"},"trusted":true},"outputs":[],"source":["# First descrimnator model\n","def make_discriminator_model():\n","    model = tf.keras.Sequential()\n","    \n","    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 3]))\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","\n","    #model.add(Flatten())\n","    model.add(Dense(3))\n","    print(model.output_shape)\n","\n","    return model"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:49.610475Z","iopub.status.busy":"2021-11-30T23:43:49.610114Z","iopub.status.idle":"2021-11-30T23:43:49.723842Z","shell.execute_reply":"2021-11-30T23:43:49.722803Z","shell.execute_reply.started":"2021-11-30T23:43:49.610435Z"},"trusted":true},"outputs":[],"source":["discriminator = make_discriminator_model()\n","decision = discriminator(generated_image)\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:43:51.333399Z","iopub.status.busy":"2021-11-30T23:43:51.333073Z","iopub.status.idle":"2021-11-30T23:43:51.342178Z","shell.execute_reply":"2021-11-30T23:43:51.340922Z","shell.execute_reply.started":"2021-11-30T23:43:51.333368Z"},"trusted":true},"outputs":[],"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:46:14.867559Z","iopub.status.busy":"2021-11-30T23:46:14.866593Z","iopub.status.idle":"2021-11-30T23:46:14.874681Z","shell.execute_reply":"2021-11-30T23:46:14.873654Z","shell.execute_reply.started":"2021-11-30T23:46:14.867504Z"},"trusted":true},"outputs":[],"source":["num_examples_to_generate = 16\n","noise_dim = 256  #WAS 100\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:46:15.427235Z","iopub.status.busy":"2021-11-30T23:46:15.426236Z","iopub.status.idle":"2021-11-30T23:46:15.437343Z","shell.execute_reply":"2021-11-30T23:46:15.436053Z","shell.execute_reply.started":"2021-11-30T23:46:15.427187Z"},"trusted":true},"outputs":[],"source":["@tf.function\n","def train_step(images):\n","  \n","    # 1 - Create a random noise to feed it into the model\n","    # for the image generation\n","    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n","    #noise = tf.random.normal([3, 256])*127.5+100\n","\n","    # 2 - Generate images and calculate loss values\n","    # GradientTape method records operations for automatic differentiation.\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","        real_output = discriminator(images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    # 3 - Calculate gradients using loss values and model variables\n","    # \"gradient\" method computes the gradient using \n","    # operations recorded in context of this tape (gen_tape and disc_tape).\n","    \n","    # It accepts a target (e.g., gen_loss) variable and \n","    # a source variable (e.g.,generator.trainable_variables)\n","    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n","    # source --> a list or nested structure of Tensors or Variables.\n","    # target will be differentiated against elements in sources.\n","\n","    # \"gradient\" method returns a list or nested structure of Tensors  \n","    # (or IndexedSlices, or None), one for each element in sources. \n","    # Returned structure is the same as the structure of sources.\n","    gradients_of_generator = gen_tape.gradient(gen_loss, \n","                                               generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n","                                                discriminator.trainable_variables)\n","    \n","    # 4 - Process  Gradients and Run the Optimizer\n","    # \"apply_gradients\" method processes aggregated gradients. \n","    # ex: optimizer.apply_gradients(zip(grads, vars))\n","    \"\"\"\n","    Example use of apply_gradients:\n","    grads = tape.gradient(loss, vars)\n","    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n","    # Processing aggregated gradients.\n","    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n","    \"\"\"\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:46:16.061493Z","iopub.status.busy":"2021-11-30T23:46:16.061161Z","iopub.status.idle":"2021-11-30T23:46:16.070590Z","shell.execute_reply":"2021-11-30T23:46:16.069364Z","shell.execute_reply.started":"2021-11-30T23:46:16.061459Z"},"trusted":true},"outputs":[],"source":["import time\n","from IPython import display # A command shell for interactive computing in Python.\n","\n","samples=[]\n","def train(dataset, epochs):\n","  # A. For each epoch, do the following:\n","  for epoch in range(epochs):\n","    print(epoch)\n","    start = time.time()\n","    # 1 - For each batch of the epoch, \n","    for image_batch in dataset:\n","      # 1.a - run the custom \"train_step\" function\n","      # we just declared above\n","        train_step(image_batch)\n","\n","    # 2 - Produce images for the GIF as we go\n","    display.clear_output(wait=True)\n","    generate_and_save_images(generator,\n","                             epoch + 1,\n","                             seed)\n","\n","    # 3 - Save the model every 5 epochs as \n","    # a checkpoint, which we will use later\n","    #if (epoch + 1) % 5 == 0:\n","     # checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    # 4 - Print out the completed epoch no. and the time spent\n","    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","    # B. Generate a final image after the training is completed\n","    display.clear_output(wait=True)\n","    generate_and_save_images(generator,\n","                           epochs,\n","                           seed)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:46:16.707444Z","iopub.status.busy":"2021-11-30T23:46:16.707160Z","iopub.status.idle":"2021-11-30T23:46:16.714245Z","shell.execute_reply":"2021-11-30T23:46:16.713071Z","shell.execute_reply.started":"2021-11-30T23:46:16.707416Z"},"trusted":true},"outputs":[],"source":["def generate_and_save_images(model, epoch, test_input):\n","    predictions = model(test_input, training=False)\n","    fig = plt.figure(figsize=(3,3))\n","    samples.append(predictions[0][0])\n","    for i in range(predictions.shape[0]):\n","        plt.subplot(4, 4, i+1)\n","        plt.imshow(predictions[i,:,:,0:3]*127.5)\n","        plt.axis('off')\n","        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n","        plt.show()\n","    "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:46:22.171225Z","iopub.status.busy":"2021-11-30T23:46:22.170878Z","iopub.status.idle":"2021-11-30T23:47:10.944148Z","shell.execute_reply":"2021-11-30T23:47:10.942792Z","shell.execute_reply.started":"2021-11-30T23:46:22.171182Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","EPOCHS = 5\n","\n","train(monet_ds, EPOCHS)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:48:08.139234Z","iopub.status.busy":"2021-11-30T23:48:08.137978Z","iopub.status.idle":"2021-11-30T23:48:08.541892Z","shell.execute_reply":"2021-11-30T23:48:08.540977Z","shell.execute_reply.started":"2021-11-30T23:48:08.139174Z"},"trusted":true},"outputs":[],"source":["generator.save_weights('generator2.h5')\n","discriminator.save_weights('discriminator2.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T17:02:34.807253Z","iopub.status.busy":"2021-11-30T17:02:34.806526Z","iopub.status.idle":"2021-11-30T17:02:34.898985Z","shell.execute_reply":"2021-11-30T17:02:34.897948Z","shell.execute_reply.started":"2021-11-30T17:02:34.807152Z"},"trusted":true},"outputs":[],"source":["plt.imshow(samples[-1])"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:58:08.769285Z","iopub.status.busy":"2021-11-30T23:58:08.768311Z","iopub.status.idle":"2021-11-30T23:58:08.971902Z","shell.execute_reply":"2021-11-30T23:58:08.970248Z","shell.execute_reply.started":"2021-11-30T23:58:08.769228Z"},"trusted":true},"outputs":[],"source":["gen3=make_generator_model()\n","gen3.load_weights('generator2.h5')\n","#dis3=make_discriminator_model()\n","#dis3.load_weights('discriminator2.h5')"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2021-11-30T23:58:10.447354Z","iopub.status.busy":"2021-11-30T23:58:10.446991Z","iopub.status.idle":"2021-11-30T23:58:10.725102Z","shell.execute_reply":"2021-11-30T23:58:10.724130Z","shell.execute_reply.started":"2021-11-30T23:58:10.447317Z"},"trusted":true},"outputs":[],"source":["noise = tf.random.normal([3, 256])*127.5+100\n","plt.imshow(gen3(noise)[0,:,:,:])\n","#generate_and_save_images(gen3, 1,)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
